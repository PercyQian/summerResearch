#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import pandas as pd
import numpy as np
import pickle
import os
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

def load_trained_models(load_path="trained_models/"):
    """
    åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ã€æ ‡å‡†åŒ–å™¨å’Œæœ€ä¼˜é˜ˆå€¼
    """
    print("=== åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ ===")
    
    models_dict = {}
    model_files = {
        'Random Forest': 'random_forest_model.pkl',
        'XGBoost': 'xgboost_model.pkl',
        'SVC': 'svc_model.pkl',
        'Logistic Regression': 'logistic_regression_model.pkl'
    }
    
    # åŠ è½½æ¨¡å‹
    for model_name, filename in model_files.items():
        model_path = os.path.join(load_path, filename)
        if os.path.exists(model_path):
            with open(model_path, 'rb') as f:
                models_dict[model_name] = pickle.load(f)
            print(f"âœ… {model_name} æ¨¡å‹å·²åŠ è½½")
        else:
            print(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
    
    # åŠ è½½æ ‡å‡†åŒ–å™¨
    scaler_path = os.path.join(load_path, 'scaler.pkl')
    scaler = None
    if os.path.exists(scaler_path):
        with open(scaler_path, 'rb') as f:
            scaler = pickle.load(f)
        print(f"âœ… æ ‡å‡†åŒ–å™¨å·²åŠ è½½")
    else:
        print(f"âš ï¸ æ ‡å‡†åŒ–å™¨æ–‡ä»¶ä¸å­˜åœ¨: {scaler_path}")
    
    # åŠ è½½æœ€ä¼˜é˜ˆå€¼
    thresholds_path = os.path.join(load_path, 'optimal_thresholds.pkl')
    optimal_thresholds = None
    if os.path.exists(thresholds_path):
        with open(thresholds_path, 'rb') as f:
            optimal_thresholds = pickle.load(f)
        print(f"âœ… æœ€ä¼˜é˜ˆå€¼å·²åŠ è½½")
    else:
        print(f"âš ï¸ æœ€ä¼˜é˜ˆå€¼æ–‡ä»¶ä¸å­˜åœ¨: {thresholds_path}")
    
    return models_dict, scaler, optimal_thresholds

def validate_model_with_new_data(processed_data_path="processed_new_data/", 
                                 models_path="trained_models/"):
    """
    ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹éªŒè¯æ–°æ•°æ®
    """
    print("=== ä½¿ç”¨è®­ç»ƒæ¨¡å‹éªŒè¯æ–°æ•°æ® ===")
    
    # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    models_dict, scaler, optimal_thresholds = load_trained_models(models_path)
    
    if not models_dict or scaler is None:
        print("âŒ æ— æ³•åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹")
        return None
    
    # åŠ è½½å¤„ç†å¥½çš„æ–°æ•°æ®
    complete_data_file = os.path.join(processed_data_path, "processed_new_data_complete.pkl")
    
    if not os.path.exists(complete_data_file):
        print(f"âŒ æ²¡æœ‰æ‰¾åˆ°å®Œæ•´æ•°æ®æ–‡ä»¶: {complete_data_file}")
        return None
    
    all_results = {}
    all_predictions = []
    
    try:
        # ç›´æ¥è¯»å–å®Œæ•´çš„å¤„ç†æ•°æ®
        new_data = pd.read_pickle(complete_data_file)
        print(f"\nğŸ“Š éªŒè¯å®Œæ•´æ•°æ®é›†: {len(new_data)} æ¡è®°å½•")
        
        if len(new_data) == 0:
            print(f"âš ï¸ æ•°æ®ä¸ºç©º")
            return None
        
        # ä½¿ç”¨ä¸è®­ç»ƒæ—¶å®Œå…¨ç›¸åŒçš„ç‰¹å¾åˆ—è¡¨å’Œé¡ºåº
        inputs = [
            'lagged_lmp_da', 'lagged_delta',
            'apparent_temperature (Â°C)', 'wind_gusts_10m (km/h)', 'pressure_msl (hPa)',
            'soil_temperature_0_to_7cm (Â°C)', 'soil_moisture_0_to_7cm (mÂ³/mÂ³)',
            'isHoliday',
            'total_lmp_da_lag_1', 'total_lmp_da_lag_2', 'total_lmp_da_lag_3',
            'total_lmp_da_lag_4', 'total_lmp_da_lag_5', 'total_lmp_da_lag_6', 'total_lmp_da_lag_7',
            'hour_10AM', 'hour_10PM', 'hour_11AM', 'hour_11PM', 'hour_12AM', 'hour_12PM',
            'hour_1AM', 'hour_1PM', 'hour_2AM', 'hour_2PM', 'hour_3AM', 'hour_3PM',
            'hour_4AM', 'hour_4PM', 'hour_5AM', 'hour_5PM', 'hour_6AM', 'hour_6PM',
            'hour_7AM', 'hour_7PM', 'hour_8AM', 'hour_8PM', 'hour_9AM', 'hour_9PM'
        ]
        
        # æ£€æŸ¥ç‰¹å¾æ˜¯å¦å­˜åœ¨ï¼Œå¯¹äºç¼ºå¤±çš„ç‰¹å¾ç”¨0å¡«å……
        available_inputs = []
        for col in inputs:
            if col in new_data.columns:
                available_inputs.append(col)
            else:
                # å¯¹äºç¼ºå¤±çš„ç‰¹å¾ï¼Œæ·»åŠ é»˜è®¤å€¼ï¼ˆä¸»è¦æ˜¯å°æ—¶ç¼–ç ï¼‰
                if col.startswith('hour_'):
                    new_data[col] = 0
                    available_inputs.append(col)
                else:
                    print(f"âš ï¸ ç¼ºå¤±ç‰¹å¾: {col}")
        
        print(f"å¯ç”¨ç‰¹å¾: {len(available_inputs)}/{len(inputs)}")
        
        if len(available_inputs) < len(inputs) * 0.7:  # è‡³å°‘70%çš„ç‰¹å¾å¯ç”¨
            print(f"âš ï¸ å¯ç”¨ç‰¹å¾ä¸è¶³ï¼Œè·³è¿‡é¢„æµ‹")
            return None
        
        # ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„ç‰¹å¾é¡ºåº
        X_new = new_data[inputs]  # ç°åœ¨æ‰€æœ‰ç‰¹å¾éƒ½åº”è¯¥å­˜åœ¨
        
        # æ£€æŸ¥æ˜¯å¦æœ‰NaNå€¼
        if X_new.isnull().any().any():
            print(f"âš ï¸ æ£€æµ‹åˆ°NaNå€¼ï¼Œç”¨0å¡«å……")
            X_new = X_new.fillna(0)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        X_new_scaled = scaler.transform(X_new)
        y_true = new_data['target_c']
        
        complete_results = {}
        
        # ä½¿ç”¨æ¯ä¸ªæ¨¡å‹è¿›è¡Œé¢„æµ‹
        for model_name, model in models_dict.items():
            try:
                # è·å–é¢„æµ‹æ¦‚ç‡
                y_pred_proba = model.predict_proba(X_new_scaled)[:, 1]
                
                # ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼
                threshold = optimal_thresholds.get(model_name, 0.5) if optimal_thresholds else 0.5
                y_pred = (y_pred_proba >= threshold).astype(int)
                
                # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
                accuracy = accuracy_score(y_true, y_pred)
                precision = precision_score(y_true, y_pred, zero_division=0)
                recall = recall_score(y_true, y_pred, zero_division=0)
                f1 = f1_score(y_true, y_pred, zero_division=0)
                auc = roc_auc_score(y_true, y_pred_proba) if len(np.unique(y_true)) > 1 else 0
                
                complete_results[model_name] = {
                    'accuracy': accuracy,
                    'precision': precision,
                    'recall': recall,
                    'f1': f1,
                    'auc': auc,
                    'predictions': y_pred,
                    'probabilities': y_pred_proba,
                    'threshold': threshold
                }
                
                print(f"{model_name}: å‡†ç¡®ç‡={accuracy:.3f}, F1={f1:.3f}, AUC={auc:.3f}")
                print(f"  é¢„æµ‹å¼‚å¸¸: {y_pred.sum()}/{len(y_pred)} ({y_pred.mean()*100:.1f}%)")
                print(f"  çœŸå®å¼‚å¸¸: {y_true.sum()}/{len(y_true)} ({y_true.mean()*100:.1f}%)")
                
                # ä¿å­˜é¢„æµ‹ç»“æœ
                prediction_record = {
                    'dataset': 'complete',
                    'model': model_name,
                    'accuracy': accuracy,
                    'precision': precision,
                    'recall': recall,
                    'f1': f1,
                    'auc': auc,
                    'threshold': threshold,
                    'num_samples': len(y_true),
                    'num_anomalies_true': y_true.sum(),
                    'num_anomalies_pred': y_pred.sum()
                }
                all_predictions.append(prediction_record)
                
            except Exception as e:
                print(f"âŒ {model_name} é¢„æµ‹å¤±è´¥: {e}")
        
        all_results['complete'] = complete_results
        
    except Exception as e:
        print(f"âŒ å¤„ç†å®Œæ•´æ•°æ®å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None
    
    # ä¿å­˜éªŒè¯ç»“æœ
    if all_predictions:
        results_df = pd.DataFrame(all_predictions)
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = f"model_validation_results_{timestamp}.csv"
        results_df.to_csv(results_file, index=False)
        print(f"\nğŸ’¾ éªŒè¯ç»“æœå·²ä¿å­˜: {results_file}")
        
        # æ˜¾ç¤ºæ€»ä½“ç»“æœæ‘˜è¦
        print(f"\nğŸ“ˆ éªŒè¯ç»“æœæ‘˜è¦:")
        summary = results_df.groupby('model').agg({
            'accuracy': 'mean',
            'precision': 'mean', 
            'recall': 'mean',
            'f1': 'mean',
            'auc': 'mean'
        }).round(3)
        print(summary)
    
    return all_results

if __name__ == "__main__":
    # è¿è¡Œæ¨¡å‹éªŒè¯
    print("ğŸš€ å¼€å§‹æ¨¡å‹éªŒè¯æµç¨‹...")
    
    validation_results = validate_model_with_new_data()
    
    if validation_results:
        print(f"\nğŸ¯ æ¨¡å‹éªŒè¯å®Œæˆï¼")
        print("- å·²ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹")
        print("- éªŒè¯ç»“æœå·²ä¿å­˜åˆ°CSVæ–‡ä»¶")
        print("- å¯ä»¥æŸ¥çœ‹å„æ¨¡å‹åœ¨æ–°æ•°æ®ä¸Šçš„è¡¨ç°")
    else:
        print("âŒ æ¨¡å‹éªŒè¯å¤±è´¥")
    
    print("\nğŸ’¡ è¯´æ˜ï¼š")
    print("- è¿™ä¸ªæµç¨‹ä½¿ç”¨äº†ä¹‹å‰è®­ç»ƒå¥½çš„4ä¸ªæ¨¡å‹")
    print("- å¯¹25æ¡æ–°çš„LMPæ•°æ®è¿›è¡Œå¼‚å¸¸æ£€æµ‹é¢„æµ‹")
    print("- è¾“å‡ºå„æ¨¡å‹çš„å‡†ç¡®ç‡ã€F1åˆ†æ•°ç­‰æŒ‡æ ‡") 