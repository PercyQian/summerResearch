#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import pandas as pd
import numpy as np
import os
from datetime import datetime

# å†å²åŸºå‡†å¼‚å¸¸æ£€æµ‹å‡½æ•°
def build_historical_baseline(historical_lmp_path="hourlyLmpData/westernData", 
                             historical_weather_path="hourlyWeatherData/openMeteo", 
                             k=2.5):
    """
    ä½¿ç”¨å†å²2-3å¹´æ•°æ®å»ºç«‹å¼‚å¸¸æ£€æµ‹åŸºå‡†
    """
    print("=== å»ºç«‹å†å²æ•°æ®å¼‚å¸¸æ£€æµ‹åŸºå‡† ===")
    
    # æ£€æŸ¥å†å²æ•°æ®æ˜¯å¦å­˜åœ¨
    if not os.path.exists(historical_lmp_path):
        print(f"å†å²LMPæ•°æ®è·¯å¾„ä¸å­˜åœ¨: {historical_lmp_path}")
        return None
        
    if not os.path.exists(historical_weather_path):
        print(f"å†å²å¤©æ°”æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {historical_weather_path}")
        return None
    
    # æ£€æŸ¥å¤©æ°”æ•°æ®æ–‡ä»¶æ ¼å¼
    weather_files = os.listdir(historical_weather_path)
    weather_files = [f for f in weather_files if f.startswith('weather_data_')]
    
    # åˆ¤æ–­æ–‡ä»¶æ ¼å¼
    has_csv = any(f.endswith('.csv') for f in weather_files)
    has_pkl = any(f.endswith('.pkl') for f in weather_files)
    
    print(f"å¤©æ°”æ•°æ®æ–‡ä»¶: CSV={has_csv}, PKL={has_pkl}")
    weather_ext = '.csv' if has_csv else '.pkl'
    
    # åˆå¹¶æ‰€æœ‰å†å²æ•°æ®
    all_historical_data = []
    lmp_files = os.listdir(historical_lmp_path)
    processed_count = 0
    
    for lmp_file in lmp_files:
        if lmp_file.startswith('lmp_data_') and lmp_file.endswith('.csv'):
            hour = lmp_file.replace('lmp_data_', '').replace('.csv', '')
            
            # è¯»å–LMPæ•°æ®
            lmp_path = os.path.join(historical_lmp_path, lmp_file)
            
            try:
                lmp_data = pd.read_pickle(lmp_path)
                print(f"ğŸ“ {hour}: è¯»å–pickleæ ¼å¼LMPæ–‡ä»¶ ({len(lmp_data)} æ¡è®°å½•)")
            except:
                try:
                    lmp_data = pd.read_csv(lmp_path)
                    print(f"ğŸ“ {hour}: è¯»å–CSVæ ¼å¼LMPæ–‡ä»¶ ({len(lmp_data)} æ¡è®°å½•)")
                except Exception as e:
                    print(f"âŒ æ— æ³•è¯»å–LMPæ–‡ä»¶ {lmp_file}: {e}")
                    continue
            
            # è¯»å–å¯¹åº”çš„å¤©æ°”æ•°æ®
            weather_file = f"weather_data_{hour}{weather_ext}"
            weather_path = os.path.join(historical_weather_path, weather_file)
            
            if os.path.exists(weather_path):
                try:
                    try:
                        weather_data = pd.read_pickle(weather_path)
                        print(f"ğŸŒ¤ï¸ {hour}: è¯»å–pickleæ ¼å¼å¤©æ°”æ–‡ä»¶ ({len(weather_data)} æ¡è®°å½•)")
                    except:
                        weather_data = pd.read_csv(weather_path)
                        print(f"ğŸŒ¤ï¸ {hour}: è¯»å–CSVæ ¼å¼å¤©æ°”æ–‡ä»¶ ({len(weather_data)} æ¡è®°å½•)")
                    
                    # ç®€åŒ–çš„æ•°æ®åˆå¹¶ï¼ˆä¸ä¾èµ–å¤æ‚çš„combineDataFrameså‡½æ•°ï¼‰
                    min_len = min(len(lmp_data), len(weather_data))
                    combined = lmp_data[:min_len].copy()
                    
                    # æ·»åŠ å¤©æ°”ç‰¹å¾
                    weather_cols = ['apparent_temperature (Â°C)', 'wind_gusts_10m (km/h)', 'pressure_msl (hPa)',
                                   'soil_temperature_0_to_7cm (Â°C)', 'soil_moisture_0_to_7cm (mÂ³/mÂ³)']
                    for col in weather_cols:
                        if col in weather_data.columns:
                            combined[col] = weather_data[col][:min_len].values
                    
                    combined['hour'] = hour
                    
                    # è®¡ç®—è¯¯å·®
                    combined['error'] = combined['total_lmp_rt'] - combined['total_lmp_da']
                    
                    # æå–æ—¶é—´ç‰¹å¾
                    if combined['datetime_beginning_utc'].dtype == 'object':
                        combined['datetime_beginning_utc'] = pd.to_datetime(combined['datetime_beginning_utc'], format='%m/%d/%Y %I:%M:%S %p')
                    
                    combined['hour_num'] = combined['datetime_beginning_utc'].dt.hour
                    combined['dow'] = combined['datetime_beginning_utc'].dt.dayofweek
                    
                    all_historical_data.append(combined)
                    processed_count += 1
                    print(f"âœ… å·²å¤„ç† {hour}: {len(combined)} æ¡è®°å½•")
                    
                except Exception as e:
                    print(f"âŒ è¯»å–å¤©æ°”æ–‡ä»¶å¤±è´¥ {weather_file}: {e}")
                    continue
                    
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°å¯¹åº”å¤©æ°”æ–‡ä»¶: {weather_path}")
    
    print(f"\nğŸ“Š å¤„ç†ç»“æœ: æˆåŠŸå¤„ç†äº† {processed_count} ä¸ªæ—¶æ®µçš„æ•°æ®")
    
    if not all_historical_data:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å†å²æ•°æ®")
        return None
    
    # åˆå¹¶æ‰€æœ‰å†å²æ•°æ®
    print("åˆå¹¶å†å²æ•°æ®...")
    historical_df = pd.concat(all_historical_data, axis=0, ignore_index=True)
    
    print(f"âœ… å†å²æ•°æ®æ€»è®¡: {len(historical_df)} æ¡è®°å½•")
    print(f"æ—¶é—´è·¨åº¦: {historical_df['datetime_beginning_utc'].min()} åˆ° {historical_df['datetime_beginning_utc'].max()}")
    print(f"è¯¯å·®ç»Ÿè®¡: å‡å€¼={historical_df['error'].mean():.3f}, æ ‡å‡†å·®={historical_df['error'].std():.3f}")
    
    # è®¡ç®—æ¯ä¸ªæ—¶æ®µçš„ç»Ÿè®¡åŸºå‡†
    print("è®¡ç®—æ—¶æ®µåŸºå‡†ç»Ÿè®¡é‡...")
    baseline_stats = historical_df.groupby(['hour_num', 'dow'])['error'].agg(
        mu='mean',
        sigma='std', 
        count='count'
    ).reset_index()
    
    # å¤„ç†æ ‡å‡†å·®ä¸ºNaNæˆ–0çš„æƒ…å†µ
    global_sigma = historical_df['error'].std()
    baseline_stats['sigma'] = baseline_stats['sigma'].fillna(global_sigma)
    baseline_stats.loc[baseline_stats['sigma'] == 0, 'sigma'] = global_sigma
    
    print(f"âœ… å»ºç«‹äº† {len(baseline_stats)} ä¸ªæ—¶æ®µåŸºå‡†")
    print("åŸºå‡†ç»Ÿè®¡é‡ç¤ºä¾‹:")
    print(baseline_stats.head(10))
    
    return baseline_stats

def detect_anomalies_with_baseline(new_data, baseline_stats, k=2.5):
    """
    ä½¿ç”¨å†å²åŸºå‡†æ£€æµ‹æ–°æ•°æ®ä¸­çš„å¼‚å¸¸
    """
    print(f"=== ä½¿ç”¨å†å²åŸºå‡†æ£€æµ‹æ–°æ•°æ®å¼‚å¸¸ (k={k}) ===")
    
    df = new_data.copy()
    
    # è®¡ç®—è¯¯å·®
    df['error'] = df['total_lmp_rt'] - df['total_lmp_da']
    
    # æå–æ—¶é—´ç‰¹å¾
    try:
        if df['datetime_beginning_utc'].dtype == 'object':
            df['datetime_beginning_utc'] = pd.to_datetime(df['datetime_beginning_utc'])
        
        df['hour_num'] = df['datetime_beginning_utc'].dt.hour
        df['dow'] = df['datetime_beginning_utc'].dt.dayofweek
    except Exception as e:
        print(f"âš ï¸ æ—¶é—´è§£æå¤±è´¥ï¼Œä½¿ç”¨hourTimeæå–å°æ—¶: {e}")
        if 'hourTime' in df.columns:
            hour_mapping = {
                '12AM': 0, '1AM': 1, '2AM': 2, '3AM': 3, '4AM': 4, '5AM': 5, '6AM': 6, '7AM': 7, '8AM': 8, '9AM': 9, '10AM': 10, '11AM': 11,
                '12PM': 12, '1PM': 13, '2PM': 14, '3PM': 15, '4PM': 16, '5PM': 17, '6PM': 18, '7PM': 19, '8PM': 20, '9PM': 21, '10PM': 22, '11PM': 23
            }
            df['hour_num'] = df['hourTime'].map(hour_mapping).fillna(0).astype(int)
            df['dow'] = 0  # å‡è®¾ä¸ºå‘¨ä¸€
    
    print(f"æ–°æ•°æ®: {len(df)} æ¡è®°å½•")
    print(f"æ—¶é—´è·¨åº¦: {df['datetime_beginning_utc'].min()} åˆ° {df['datetime_beginning_utc'].max()}")
    
    # åˆå¹¶åŸºå‡†ç»Ÿè®¡é‡
    df = df.merge(baseline_stats, on=['hour_num', 'dow'], how='left')
    
    # å¯¹äºæ²¡æœ‰å†å²åŸºå‡†çš„æ—¶æ®µï¼Œä½¿ç”¨å…¨å±€ç»Ÿè®¡é‡
    global_mu = baseline_stats['mu'].mean()
    global_sigma = baseline_stats['sigma'].mean()
    
    df['mu'] = df['mu'].fillna(global_mu)
    df['sigma'] = df['sigma'].fillna(global_sigma)
    
    # è®¡ç®—å¼‚å¸¸
    df['abs_deviation'] = np.abs(df['error'] - df['mu'])
    df['threshold'] = k * df['sigma']
    df['is_anomaly'] = (df['abs_deviation'] > df['threshold']).astype(int)
    
    # ç»Ÿè®¡ç»“æœ
    total_anomalies = df['is_anomaly'].sum()
    anomaly_rate = total_anomalies / len(df) * 100
    
    print(f"\nå¼‚å¸¸æ£€æµ‹ç»“æœ:")
    print(f"æ€»å¼‚å¸¸æ•°é‡: {total_anomalies}")
    print(f"å¼‚å¸¸ç‡: {anomaly_rate:.2f}%")
    
    # æ˜¾ç¤ºå¼‚å¸¸è¯¦æƒ…
    if total_anomalies > 0:
        anomalies = df[df['is_anomaly'] == 1].sort_values('abs_deviation', ascending=False)
        print(f"\nå‰{min(5, len(anomalies))}ä¸ªæœ€å¤§å¼‚å¸¸:")
        cols = ['datetime_beginning_utc', 'error', 'abs_deviation', 'threshold', 'hour_num', 'dow']
        print(anomalies[cols].head(5))
    else:
        print("\næ²¡æœ‰æ£€æµ‹åˆ°å¼‚å¸¸")
    
    return df

def total_lmp_delta(data):
    """è®¡ç®—LMPåå·®"""
    data['total_lmp_delta'] = 0
    for i in list(data.index):
        a = data.loc[i, "total_lmp_da"]
        r = data.loc[i, "total_lmp_rt"]
        if abs(a) < 1e-6:
            a = 1e-6
        data.loc[i, "total_lmp_delta"] = abs((a - r) / a)

def applyHoliday(data, holidays):
    """æ·»åŠ å‡æ—¥æ ‡è®°"""
    data['isHoliday'] = 0  # ç®€åŒ–ç‰ˆï¼šå…¨éƒ¨è®¾ä¸ºéå‡æ—¥
    return data

def process_new_lmp_data_complete(da_file, rt_file, weather_file=None, output_path="processed_new_data/"):
    """
    å¤„ç†æ–°çš„LMPæ•°æ®ï¼ˆå®Œæ•´ç‰ˆï¼šç›´æ¥å¤„ç†æ—¶é—´åºåˆ—æ•°æ®ï¼Œç±»ä¼¼è®­ç»ƒæ—¶çš„all_dataï¼‰
    """
    import os
    if not os.path.exists(output_path):
        os.makedirs(output_path)
    
    print("=== å¤„ç†æ–°çš„LMPæ•°æ®ï¼ˆå®Œæ•´ç‰ˆï¼‰===")
    
    # è¯»å–LMPæ•°æ®
    try:
        da_data = pd.read_csv(da_file)
        rt_data = pd.read_csv(rt_file)
        print(f"DAæ•°æ®: {len(da_data)} è¡Œ")
        print(f"RTæ•°æ®: {len(rt_data)} è¡Œ")
    except Exception as e:
        print(f"âŒ è¯»å–LMPæ•°æ®å¤±è´¥: {e}")
        return None
    
    # åˆå¹¶DAå’ŒRTæ•°æ®
    if len(da_data) != len(rt_data):
        print("âš ï¸ DAå’ŒRTæ•°æ®è¡Œæ•°ä¸åŒ¹é…ï¼Œå–è¾ƒå°é•¿åº¦")
        min_len = min(len(da_data), len(rt_data))
        da_data = da_data[:min_len].reset_index(drop=True)
        rt_data = rt_data[:min_len].reset_index(drop=True)
    
    # åˆå¹¶DAå’ŒRTæ•°æ®
    combined_lmp = da_data.copy()
    if 'total_lmp_rt' not in combined_lmp.columns:
        rt_col = 'total_lmp_rt' if 'total_lmp_rt' in rt_data.columns else rt_data.columns[-1]
        combined_lmp['total_lmp_rt'] = rt_data[rt_col]
    
    print(f"åˆå¹¶åLMPæ•°æ®: {len(combined_lmp)} è¡Œ")
    print(f"æ•°æ®åˆ—: {combined_lmp.columns.tolist()}")
    
    # ä»æ—¶é—´åˆ—æå–å°æ—¶ä¿¡æ¯å¹¶åˆ›å»ºone-hotç¼–ç ï¼ˆç±»ä¼¼è®­ç»ƒæ—¶çš„å¤„ç†ï¼‰
    print("æå–å°æ—¶ä¿¡æ¯å¹¶åˆ›å»ºç¼–ç ...")
    
    # è§£ææ—¶é—´å¹¶æå–å°æ—¶
    try:
        # å°è¯•è§£ææ—¶é—´æ ¼å¼
        combined_lmp['datetime_parsed'] = pd.to_datetime(combined_lmp['datetime_beginning_utc'])
        combined_lmp['hour_24'] = combined_lmp['datetime_parsed'].dt.hour
        
        # è½¬æ¢ä¸º12å°æ—¶åˆ¶æ ¼å¼ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
        def convert_to_12h_format(hour_24):
            if hour_24 == 0:
                return "12AM"
            elif hour_24 < 12:
                return f"{hour_24}AM"
            elif hour_24 == 12:
                return "12PM"
            else:
                return f"{hour_24-12}PM"
        
        combined_lmp['hourTime'] = combined_lmp['hour_24'].apply(convert_to_12h_format)
        
        # æ˜¾ç¤ºå°æ—¶åˆ†å¸ƒ
        hour_counts = combined_lmp['hourTime'].value_counts()
        print(f"å°æ—¶åˆ†å¸ƒ: {hour_counts.to_dict()}")
        
    except Exception as e:
        print(f"âš ï¸ æ—¶é—´è§£æå¤±è´¥: {e}")
        # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
        combined_lmp['hourTime'] = "4AM"  # é»˜è®¤å°æ—¶
    
    # æ·»åŠ å°æ—¶one-hotç¼–ç ï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰
    print("åˆ›å»ºå°æ—¶one-hotç¼–ç ...")
    all_hours = ['12AM', '1AM', '2AM', '3AM', '4AM', '5AM', '6AM', '7AM', '8AM', '9AM', '10AM', '11AM',
                '12PM', '1PM', '2PM', '3PM', '4PM', '5PM', '6PM', '7PM', '8PM', '9PM', '10PM', '11PM']
    
    # ä½¿ç”¨pandas get_dummiesåˆ›å»ºone-hotç¼–ç ï¼ˆæ›´é«˜æ•ˆï¼‰
    hour_dummies = pd.get_dummies(combined_lmp['hourTime'], prefix='hour')
    
    # ç¡®ä¿æ‰€æœ‰24å°æ—¶éƒ½æœ‰å¯¹åº”çš„åˆ—ï¼ˆå³ä½¿æ•°æ®ä¸­æ²¡æœ‰ï¼‰
    for h in all_hours:
        col_name = f'hour_{h}'
        if col_name not in hour_dummies.columns:
            hour_dummies[col_name] = 0
    
    # æŒ‰æ­£ç¡®é¡ºåºæ’åˆ—å°æ—¶åˆ—
    hour_cols = [f'hour_{h}' for h in all_hours]
    combined_lmp = pd.concat([combined_lmp, hour_dummies[hour_cols]], axis=1)
    
    # æ·»åŠ å¤©æ°”ç‰¹å¾ï¼ˆä½¿ç”¨å®é™…å¤©æ°”æ•°æ®ï¼‰
    print("æ·»åŠ å¤©æ°”ç‰¹å¾...")
    
    if weather_file and os.path.exists(weather_file):
        try:
            # è¯»å–å¤©æ°”æ•°æ®
            if weather_file.endswith('.pkl'):
                weather_data = pd.read_pickle(weather_file)
            else:
                weather_data = pd.read_csv(weather_file)
            
            print(f"åŠ è½½å¤©æ°”æ•°æ®: {len(weather_data)} è¡Œ")
            
            # è§£æå¤©æ°”æ•°æ®çš„æ—¶é—´
            weather_data['datetime_parsed'] = pd.to_datetime(weather_data['time'])
            weather_data['datetime_rounded'] = weather_data['datetime_parsed'].dt.round('H')
            
            # è§£æLMPæ•°æ®çš„æ—¶é—´å¹¶å››èˆäº”å…¥åˆ°å°æ—¶
            combined_lmp['datetime_rounded'] = pd.to_datetime(combined_lmp['datetime_beginning_utc']).dt.round('H')
            
            # æŒ‰æ—¶é—´åŒ¹é…å¤©æ°”æ•°æ®
            weather_cols = [
                'apparent_temperature (Â°C)', 
                'wind_gusts_10m (km/h)', 
                'pressure_msl (hPa)',
                'soil_temperature_0_to_7cm (Â°C)', 
                'soil_moisture_0_to_7cm (mÂ³/mÂ³)'
            ]
            
            # åˆ›å»ºå¤©æ°”æ•°æ®çš„æŸ¥æ‰¾å­—å…¸
            weather_lookup = {}
            for _, row in weather_data.iterrows():
                time_key = row['datetime_rounded']
                weather_lookup[time_key] = {col: row[col] for col in weather_cols if col in row}
            
            # ä¸ºæ¯è¡ŒLMPæ•°æ®åŒ¹é…å¤©æ°”æ•°æ®
            for col in weather_cols:
                combined_lmp[col] = 0  # åˆå§‹åŒ–
            
            matched_count = 0
            for idx, row in combined_lmp.iterrows():
                time_key = row['datetime_rounded']
                if time_key in weather_lookup:
                    for col in weather_cols:
                        if col in weather_lookup[time_key]:
                            combined_lmp.loc[idx, col] = weather_lookup[time_key][col]
                    matched_count += 1
                else:
                    # å¦‚æœæ²¡æœ‰ç²¾ç¡®åŒ¹é…ï¼Œä½¿ç”¨æœ€è¿‘çš„å¤©æ°”æ•°æ®
                    closest_time = min(weather_lookup.keys(), key=lambda x: abs((x - time_key).total_seconds()))
                    for col in weather_cols:
                        if col in weather_lookup[closest_time]:
                            combined_lmp.loc[idx, col] = weather_lookup[closest_time][col]
            
            print(f"âœ… å¤©æ°”ç‰¹å¾æ·»åŠ å®Œæˆï¼š{matched_count}/{len(combined_lmp)} æ¡è®°å½•ç²¾ç¡®åŒ¹é…")
            
        except Exception as e:
            print(f"âš ï¸ è¯»å–å¤©æ°”æ•°æ®å¤±è´¥: {e}")
            print("ä½¿ç”¨é»˜è®¤å¤©æ°”å€¼...")
            # ä½¿ç”¨é»˜è®¤å€¼ä½œä¸ºå¤‡é€‰
            default_weather_values = {
                'apparent_temperature (Â°C)': 15.0,
                'wind_gusts_10m (km/h)': 20.0,
                'pressure_msl (hPa)': 1013.25,
                'soil_temperature_0_to_7cm (Â°C)': 12.0,
                'soil_moisture_0_to_7cm (mÂ³/mÂ³)': 0.3
            }
            for col, val in default_weather_values.items():
                combined_lmp[col] = val
    else:
        print("âš ï¸ æ²¡æœ‰æä¾›å¤©æ°”æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤å€¼")
        # ä½¿ç”¨é»˜è®¤å€¼
        default_weather_values = {
            'apparent_temperature (Â°C)': 15.0,
            'wind_gusts_10m (km/h)': 20.0,
            'pressure_msl (hPa)': 1013.25,
            'soil_temperature_0_to_7cm (Â°C)': 12.0,
            'soil_moisture_0_to_7cm (mÂ³/mÂ³)': 0.3
        }
        for col, val in default_weather_values.items():
            combined_lmp[col] = val
    
    # æ·»åŠ å…¶ä»–ç‰¹å¾
    print("æ·»åŠ å…¶ä»–ç‰¹å¾...")
    
    # å‡æ—¥ç‰¹å¾
    combined_lmp = applyHoliday(combined_lmp, [])
    
    # è®¡ç®—LMPåå·®
    total_lmp_delta(combined_lmp)
    
    # æ·»åŠ æ»åç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    combined_lmp['lagged_lmp_da'] = combined_lmp['total_lmp_da'].shift(1).fillna(combined_lmp['total_lmp_da'].iloc[0])
    combined_lmp['lagged_lmp_rt'] = combined_lmp['total_lmp_rt'].shift(1).fillna(combined_lmp['total_lmp_rt'].iloc[0])
    combined_lmp['lagged_delta'] = combined_lmp['total_lmp_delta'].shift(1).fillna(0)
    
    # æ·»åŠ å¤šä¸ªæ»åç‰¹å¾ï¼ˆå¦‚æœè®­ç»ƒæ—¶ä½¿ç”¨äº†ï¼‰
    k = 7  # ä¸è®­ç»ƒæ—¶ä¸€è‡´
    for i in range(1, k+1):
        combined_lmp[f'total_lmp_da_lag_{i}'] = combined_lmp['total_lmp_da'].shift(i).fillna(combined_lmp['total_lmp_da'].iloc[0])
    
    # ä½¿ç”¨å†å²åŸºå‡†çš„å¼‚å¸¸æ£€æµ‹ï¼ˆåŸºäºæ—¶é—´å’Œæ˜ŸæœŸçš„ç›¸å¯¹å·®å¼‚ï¼‰
    print("ä½¿ç”¨å†å²åŸºå‡†æ£€æµ‹å¼‚å¸¸...")
    try:
        # å»ºç«‹å†å²åŸºå‡†
        baseline_stats = build_historical_baseline(k=2.5)
        
        if baseline_stats is not None:
            # ä½¿ç”¨å†å²åŸºå‡†æ£€æµ‹å¼‚å¸¸
            combined_lmp = detect_anomalies_with_baseline(combined_lmp, baseline_stats, k=2.5)
            
            # å°†å¼‚å¸¸æ£€æµ‹ç»“æœè½¬æ¢ä¸ºtarget_cæ ¼å¼
            combined_lmp['target_c'] = combined_lmp['is_anomaly'].astype(int)
            combined_lmp['target_r'] = combined_lmp['total_lmp_delta']
            
            print(f"åŸºäºå†å²åŸºå‡†çš„å¼‚å¸¸åˆ†å¸ƒ: {combined_lmp['target_c'].value_counts().to_dict()}")
        else:
            print("âš ï¸ æ— æ³•å»ºç«‹å†å²åŸºå‡†ï¼Œä½¿ç”¨ç®€åŒ–çš„å¼‚å¸¸æ£€æµ‹")
            # åŸºäºå½“å‰æ•°æ®çš„ç®€åŒ–å¼‚å¸¸æ£€æµ‹ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰
            if len(combined_lmp) > 10:
                # ä½¿ç”¨æ›´ä¸¥æ ¼çš„é˜ˆå€¼ï¼ˆ99%åˆ†ä½æ•°è€Œä¸æ˜¯95%ï¼‰
                threshold_99 = combined_lmp['total_lmp_delta'].quantile(0.99)
                combined_lmp['target_c'] = (combined_lmp['total_lmp_delta'] >= threshold_99).astype(int)
                print(f"ä½¿ç”¨99%åˆ†ä½æ•°é˜ˆå€¼: {threshold_99:.4f}")
            else:
                combined_lmp['target_c'] = 0  # æ•°æ®å¤ªå°‘ï¼Œé»˜è®¤ä¸ºéå¼‚å¸¸
            combined_lmp['target_r'] = combined_lmp['total_lmp_delta']
            print(f"ç®€åŒ–å¼‚å¸¸åˆ†å¸ƒ: {combined_lmp['target_c'].value_counts().to_dict()}")
            
    except Exception as e:
        print(f"âš ï¸ å†å²åŸºå‡†å¼‚å¸¸æ£€æµ‹å¤±è´¥: {e}")
        # fallbackåˆ°æœ€ç®€å•çš„æ–¹æ³•
        combined_lmp['target_c'] = 0  # é»˜è®¤ä¸ºéå¼‚å¸¸
        combined_lmp['target_r'] = combined_lmp['total_lmp_delta']
        print("ä½¿ç”¨é»˜è®¤å¼‚å¸¸æ ‡è®°ï¼ˆå…¨éƒ¨ä¸ºéå¼‚å¸¸ï¼‰")
    
    # ä¿å­˜å¤„ç†åçš„å®Œæ•´æ•°æ®
    output_file = os.path.join(output_path, "processed_new_data_complete.pkl")
    combined_lmp.to_pickle(output_file)
    print(f"ğŸ’¾ å®Œæ•´æ•°æ®å·²ä¿å­˜: {output_file} ({len(combined_lmp)} æ¡è®°å½•)")
    
    print(f"\nâœ… æ–°æ•°æ®å¤„ç†å®Œæˆ: {len(combined_lmp)} æ¡è®°å½•")
    print(f"åŒ…å«ç‰¹å¾: {len(combined_lmp.columns)} åˆ—")
    
    # æ˜¾ç¤ºç‰¹å¾åˆ—è¡¨
    feature_cols = [col for col in combined_lmp.columns if col.startswith(('lagged_', 'apparent_', 'wind_', 'pressure_', 'soil_', 'isHoliday', 'hour_', 'total_lmp_da_lag_'))]
    print(f"ç‰¹å¾åˆ—: {len(feature_cols)} ä¸ª")
    print(f"å‰10ä¸ªç‰¹å¾: {feature_cols[:10]}")
    
    # è¿”å›å­—å…¸æ ¼å¼ä»¥ä¿æŒå…¼å®¹æ€§
    return {"complete": combined_lmp}

if __name__ == "__main__":
    # æµ‹è¯•æ–°çš„æ•°æ®å¤„ç†æµç¨‹
    result = process_new_lmp_data_complete(
        da_file="applicationData/da_hrl_lmps_2025.csv",
        rt_file="applicationData/rt_hrl_lmps_2025.csv",
        weather_file="weatherData/meteo/western_hub_weather_2025-01-01_to_2025-06-06.pkl"
    )
    
    if result:
        print("ğŸ¯ æ•°æ®å¤„ç†æˆåŠŸï¼")
        data = result["complete"]
        print(f"æœ€ç»ˆæ•°æ®å½¢çŠ¶: {data.shape}")
        
        # æ£€æŸ¥å…³é”®ç‰¹å¾æ˜¯å¦å­˜åœ¨
        required_features = [
            'lagged_lmp_da', 'lagged_delta',
            'apparent_temperature (Â°C)', 'wind_gusts_10m (km/h)', 'pressure_msl (hPa)',
            'soil_temperature_0_to_7cm (Â°C)', 'soil_moisture_0_to_7cm (mÂ³/mÂ³)',
            'isHoliday', 'hour_4AM', 'total_lmp_da_lag_1'
        ]
        
        missing_features = [f for f in required_features if f not in data.columns]
        if missing_features:
            print(f"âš ï¸ ç¼ºå¤±ç‰¹å¾: {missing_features}")
        else:
            print("âœ… æ‰€æœ‰å¿…éœ€ç‰¹å¾éƒ½å­˜åœ¨")
    else:
        print("âŒ æ•°æ®å¤„ç†å¤±è´¥") 